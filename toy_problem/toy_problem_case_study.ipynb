{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toy Problem Case Study: Tokenization Effects in VLA Training\n",
        "\n",
        "This notebook reproduces the case study from the paper that demonstrates how naive tokenization schemes affect the training of autoregressive vision-language-action (VLA) policies at different sampling rates.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The case study reproduces the key findings from the paper showing that:\n",
        "\n",
        "1. **Naive binning tokenization** works well at low sampling rates (H=25-50)\n",
        "2. **Performance degrades significantly** at high sampling rates (H=400-800)\n",
        "3. **Marginal information content approaches zero** as sampling frequency increases\n",
        "4. **Models tend to copy the first action** at high frequencies instead of learning meaningful patterns\n",
        "\n",
        "This demonstrates the need for better tokenization schemes like the DCT-based FAST tokenization proposed in the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from typing import Dict, List\n",
        "\n",
        "# Set up matplotlib for better plots\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Import our modules\n",
        "from cubic_spline_generator import CubicSplineGenerator\n",
        "from binning_tokenizer import BinningTokenizer\n",
        "from transformer_model import SimpleTransformer, count_parameters\n",
        "from training import Trainer, run_experiment\n",
        "from visualization import CaseStudyVisualizer\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Demonstrate the Tokenization Issue\n",
        "\n",
        "First, let's demonstrate the core tokenization issue described in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize components\n",
        "generator = CubicSplineGenerator(seed=42)\n",
        "tokenizer = BinningTokenizer(num_bins=256)\n",
        "\n",
        "# Test different sampling rates\n",
        "sampling_rates = [25, 50, 100, 200, 400, 800]\n",
        "\n",
        "print(\"Analyzing marginal information content:\")\n",
        "print(\"Sampling Rate | Entropy | Zero Diff Ratio | Unique Diffs\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for H in sampling_rates:\n",
        "    # Generate data\n",
        "    times, targets, conditioning = generator.generate_spline_data(\n",
        "        num_sequences=100,\n",
        "        sequence_length=H\n",
        "    )\n",
        "    \n",
        "    # Fit tokenizer\n",
        "    tokenizer.fit(targets)\n",
        "    \n",
        "    # Analyze marginal information\n",
        "    analysis = tokenizer.analyze_marginal_information(targets, H)\n",
        "    results[H] = analysis\n",
        "    \n",
        "    print(f\"{H:13d} | {analysis['entropy']:7.3f} | {analysis['zero_diff_ratio']:13.3f} | {analysis['unique_diffs']:11d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the Tokenization Issue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Entropy vs Sampling Rate\n",
        "axes[0].plot(sampling_rates, [results[H]['entropy'] for H in sampling_rates], 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Sampling Rate (H)')\n",
        "axes[0].set_ylabel('Entropy of Token Differences')\n",
        "axes[0].set_title('Marginal Information Content')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xscale('log')\n",
        "\n",
        "# Plot 2: Zero Difference Ratio vs Sampling Rate\n",
        "axes[1].plot(sampling_rates, [results[H]['zero_diff_ratio'] for H in sampling_rates], 'ro-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Sampling Rate (H)')\n",
        "axes[1].set_ylabel('Ratio of Zero Differences')\n",
        "axes[1].set_title('Token Redundancy')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xscale('log')\n",
        "\n",
        "# Plot 3: Example spline at different sampling rates\n",
        "# Generate a single example spline\n",
        "times, targets, conditioning = generator.generate_spline_data(\n",
        "    num_sequences=1,\n",
        "    sequence_length=100  # Use medium sampling rate for visualization\n",
        ")\n",
        "\n",
        "axes[2].plot(times[0], targets[0], 'k-', linewidth=2, label='Cubic Spline')\n",
        "axes[2].scatter(conditioning[0, :, 0], conditioning[0, :, 1], \n",
        "               color='red', s=100, zorder=5, label='Conditioning Points')\n",
        "axes[2].set_xlabel('Time')\n",
        "axes[2].set_ylabel('Value')\n",
        "axes[2].set_title('Example Cubic Spline')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print key insights\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. ENTROPY DECREASES with sampling rate:\")\n",
        "print(f\"   H=25:  {results[25]['entropy']:.3f}\")\n",
        "print(f\"   H=800: {results[800]['entropy']:.3f}\")\n",
        "print(f\"   Reduction: {(1 - results[800]['entropy']/results[25]['entropy'])*100:.1f}%\")\n",
        "\n",
        "print(\"\\n2. TOKEN REDUNDANCY INCREASES with sampling rate:\")\n",
        "print(f\"   H=25:  {results[25]['zero_diff_ratio']:.3f} zero differences\")\n",
        "print(f\"   H=800: {results[800]['zero_diff_ratio']:.3f} zero differences\")\n",
        "print(f\"   Increase: {(results[800]['zero_diff_ratio']/results[25]['zero_diff_ratio'] - 1)*100:.1f}%\")\n",
        "\n",
        "print(\"\\n3. UNIQUE DIFFERENCES DECREASE with sampling rate:\")\n",
        "print(f\"   H=25:  {results[25]['unique_diffs']} unique differences\")\n",
        "print(f\"   H=800: {results[800]['unique_diffs']} unique differences\")\n",
        "print(f\"   Reduction: {(1 - results[800]['unique_diffs']/results[25]['unique_diffs'])*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run the Training Experiment\n",
        "\n",
        "Now let's run the actual training experiment to see how the model performance degrades with sampling rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create results directory\n",
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Initialize visualizer\n",
        "visualizer = CaseStudyVisualizer(device)\n",
        "\n",
        "print(\"Setup complete for training experiment!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Test with Reduced Parameters\n",
        "\n",
        "Let's start with a quick test using smaller parameters to demonstrate the effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run quick experiment\n",
        "print(\"Running quick experiment with reduced parameters...\")\n",
        "print(\"Sampling rates: [25, 100, 400]\")\n",
        "print(\"Sequences: 200, Epochs: 20\")\n",
        "\n",
        "quick_results = run_experiment(\n",
        "    sampling_rates=[25, 100, 400],\n",
        "    num_sequences=200,  # Smaller for faster testing\n",
        "    num_epochs=20,      # Fewer epochs for faster testing\n",
        "    results_dir=results_dir\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"QUICK EXPERIMENT RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Sampling Rate (H) | MSE\")\n",
        "print(\"-\" * 30)\n",
        "for H in sorted(quick_results.keys()):\n",
        "    print(f\"{H:15d} | {quick_results[H]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "sampling_rates_quick = sorted(quick_results.keys())\n",
        "mse_values = [quick_results[H] for H in sampling_rates_quick]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sampling_rates_quick, mse_values, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Sampling Rate (H)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Effect of Sampling Rate on Prediction Performance\\n(Naive Binning Tokenization)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "# Add annotations\n",
        "for H, mse in zip(sampling_rates_quick, mse_values):\n",
        "    plt.annotate(f'{mse:.2e}', (H, mse), \n",
        "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nExpected behavior:\")\n",
        "print(\"- H=25: Good performance (low MSE)\")\n",
        "print(\"- H=100: Moderate performance\")\n",
        "print(\"- H=400: Poor performance (high MSE)\")\n",
        "print(\"\\nThis demonstrates the tokenization issue described in the paper!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenization Error Analysis\n",
        "\n",
        "Let's also analyze the tokenization error itself at different sampling rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"TOKENIZATION ERROR ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sampling_rates_error = [25, 100, 400]\n",
        "\n",
        "print(\"Sampling Rate | Tokenization MSE | Relative Error\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "tokenization_errors = []\n",
        "\n",
        "for H in sampling_rates_error:\n",
        "    # Generate data\n",
        "    times, targets, conditioning = generator.generate_spline_data(\n",
        "        num_sequences=100,\n",
        "        sequence_length=H\n",
        "    )\n",
        "    \n",
        "    # Fit tokenizer\n",
        "    tokenizer.fit(targets)\n",
        "    \n",
        "    # Tokenize and detokenize\n",
        "    tokens = tokenizer.tokenize(targets)\n",
        "    reconstructed = tokenizer.detokenize(tokens)\n",
        "    \n",
        "    # Compute error\n",
        "    mse = tokenizer.compute_tokenization_error(targets, reconstructed)\n",
        "    \n",
        "    # Compute relative error\n",
        "    target_range = targets.max() - targets.min()\n",
        "    relative_error = mse / (target_range ** 2)\n",
        "    \n",
        "    tokenization_errors.append(mse)\n",
        "    \n",
        "    print(f\"{H:13d} | {mse:15.2f} | {relative_error:13.6f}\")\n",
        "\n",
        "print(\"\\nNote: Tokenization error increases with sampling rate\")\n",
        "print(\"due to the finite resolution of the 256 bins.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Full Experiment (Optional)\n",
        "\n",
        "If you want to run the full experiment with all sampling rates, uncomment and run the cell below. Note that this will take significantly longer and may require more GPU memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to run full experiment\n",
        "# print(\"Running full experiment...\")\n",
        "# print(\"This may take a while and require significant GPU memory.\")\n",
        "# \n",
        "# full_results = run_experiment(\n",
        "#     sampling_rates=[25, 50, 100, 200, 400, 800],\n",
        "#     num_sequences=1000,\n",
        "#     num_epochs=100,\n",
        "#     results_dir=results_dir\n",
        "# )\n",
        "# \n",
        "# print(\"\\n\" + \"=\" * 60)\n",
        "# print(\"FULL EXPERIMENT RESULTS\")\n",
        "# print(\"=\" * 60)\n",
        "# print(\"Sampling Rate (H) | MSE\")\n",
        "# print(\"-\" * 30)\n",
        "# for H in sorted(full_results.keys()):\n",
        "#     print(f\"{H:15d} | {full_results[H]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conclusion\n",
        "\n",
        "This case study successfully reproduces the key findings from the paper:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This case study demonstrates why naive binning tokenization fails at high sampling rates:\")\n",
        "print()\n",
        "print(\"1. MARGINAL INFORMATION PROBLEM:\")\n",
        "print(\"   - As sampling rate increases, consecutive tokens become highly correlated\")\n",
        "print(\"   - This reduces the marginal information content that autoregressive models rely on\")\n",
        "print()\n",
        "print(\"2. TOKEN REDUNDANCY:\")\n",
        "print(\"   - At high frequencies, many consecutive tokens are identical or very similar\")\n",
        "print(\"   - This makes it hard for the model to learn meaningful patterns\")\n",
        "print()\n",
        "print(\"3. COPY BEHAVIOR:\")\n",
        "print(\"   - Models trained at high sampling rates tend to simply copy the first action\")\n",
        "print(\"   - Instead of interpolating the smooth spline curve\")\n",
        "print()\n",
        "print(\"4. NEED FOR BETTER TOKENIZATION:\")\n",
        "print(\"   - This case study motivates the development of better tokenization schemes\")\n",
        "print(\"   - Like the DCT-based FAST tokenization proposed in the paper\")\n",
        "print()\n",
        "print(\"The results justify the paper's proposal for improved tokenization methods\")\n",
        "print(\"that maintain high information content across all sampling rates.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "To complete the case study, you could:\n",
        "\n",
        "1. **Implement the FAST tokenization** using DCT-based encoding\n",
        "2. **Compare performance** between naive binning and FAST tokenization\n",
        "3. **Run on real robot data** to validate the findings\n",
        "4. **Experiment with different bin sizes** and tokenization schemes\n",
        "\n",
        "The current implementation provides a solid foundation for exploring these extensions.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
